---
title: "Day 8 Flannel CNI ç¶²è·¯æ·±å…¥è§£æ"
tags: 2025éµäººè³½
date: 2025-07-20
---

# Kubernetes å­¸ç¿’ - Day 8: Flannel CNI ç¶²è·¯æ·±å…¥è§£æ

## ğŸ“š å­¸ç¿’ç›®æ¨™

ä»Šå¤©æˆ‘å€‘å°‡æ·±å…¥äº†è§£ Kubernetes çš„ç¶²è·¯æ ¸å¿ƒ - CNI (Container Network Interface)ï¼Œä¸¦å°ˆæ³¨æ–¼ Flannel CNI çš„å·¥ä½œåŸç†ã€‚æˆ‘å€‘æœƒå¾ Docker Compose çš„ç¶²è·¯æ¦‚å¿µå‡ºç™¼ï¼Œé€æ­¥ç†è§£ Kubernetes çš„ç¶²è·¯æ¨¡å‹ã€‚

### ğŸ¯ å­¸ç¿’é‡é»
- ç†è§£ CNI çš„æ¦‚å¿µèˆ‡æ¶æ§‹
- æŒæ¡ Flannel CNI çš„å·¥ä½œåŸç†
- åˆ†æç¶²è·¯å°åŒ…æµå‘
- æ¯”è¼ƒä¸åŒå¾Œç«¯æ¨¡å¼
- å¯¦ä½œè‡ªè¨‚ KinD å¢é›†ä¸¦æ¸¬è©¦ç¶²è·¯é€šè¨Š

---

## ğŸ” å¾ Docker Compose åˆ° Kubernetes ç¶²è·¯

### Docker Compose ç¶²è·¯å›é¡§

åœ¨ Docker Compose ä¸­ï¼Œç¶²è·¯ç›¸å°ç°¡å–®ï¼š

```yaml
# docker-compose.yml
version: '3.8'
services:
  web:
    image: nginx
    networks:
      - app-network
  
  api:
    image: node:14
    networks:
      - app-network
  
  db:
    image: postgres
    networks:
      - db-network

networks:
  app-network:
    driver: bridge
  db-network:
    driver: bridge
```

### Docker Compose ç¶²è·¯æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph DockerHost["Docker Host"]
        subgraph AppNetwork["app-network (bridge)"]
            WEB["Web Container<br/>172.20.0.2"]
            API["API Container<br/>172.20.0.3"]
        end
        
        subgraph DbNetwork["db-network (bridge)"]
            DB["DB Container<br/>172.21.0.2"]
            API2["API Container<br/>172.21.0.3"]
        end
        
        DOCKER0["docker0 bridge<br/>172.17.0.1"]
        HOST_ETH["Host eth0<br/>192.168.1.100"]
    end
    
    WEB -.-> API
    API -.-> API2
    API2 -.-> DB
    
    DOCKER0 --> HOST_ETH
    
    style WEB fill:#e3f2fd
    style API fill:#e8f5e8
    style DB fill:#fff3e0
    style API2 fill:#e8f5e8
```

### Docker Compose ç¶²è·¯ç‰¹é»

âœ… **å„ªé»**ï¼š
- ç°¡å–®æ˜“ç”¨ï¼Œè‡ªå‹• DNS è§£æ
- å®¹å™¨é–“å¯ç›´æ¥é€šéæœå‹™åé€šè¨Š
- ç¶²è·¯éš”é›¢æ€§å¥½

âŒ **é™åˆ¶**ï¼š
- åªèƒ½åœ¨å–®ä¸€ä¸»æ©Ÿä¸Šé‹è¡Œ
- ç„¡æ³•è·¨ä¸»æ©Ÿé€šè¨Š
- æ“´å±•æ€§æœ‰é™

---

## ğŸŒ Kubernetes ç¶²è·¯æŒ‘æˆ°

ç•¶æˆ‘å€‘å¾å–®æ©Ÿçš„ Docker Compose è½‰ç§»åˆ°å¤šç¯€é»çš„ Kubernetes æ™‚ï¼Œé¢è‡¨çš„ç¶²è·¯æŒ‘æˆ°ï¼š

### å¤šç¯€é»ç¶²è·¯éœ€æ±‚

```mermaid
graph TB
    subgraph cluster ["Kubernetes Cluster"]
        subgraph master ["Master Node"]
            MASTER["Control Plane<br/>192.168.1.10"]
        end
        
        subgraph worker1 ["Worker Node 1"]
            NODE1["kubelet<br/>192.168.1.11"]
            POD1A["Pod A<br/>10.244.1.2"]
            POD1B["Pod B<br/>10.244.1.3"]
        end
        
        subgraph worker2 ["Worker Node 2"]
            NODE2["kubelet<br/>192.168.1.12"]
            POD2A["Pod C<br/>10.244.2.2"]
            POD2B["Pod D<br/>10.244.2.3"]
        end
        
        subgraph worker3 ["Worker Node 3"]
            NODE3["kubelet<br/>192.168.1.13"]
            POD3A["Pod E<br/>10.244.3.2"]
            POD3B["Pod F<br/>10.244.3.3"]
        end
    end
    
    POD1A -.->|éœ€è¦é€šè¨Š| POD2A
    POD1B -.->|éœ€è¦é€šè¨Š| POD3A
    POD2B -.->|éœ€è¦é€šè¨Š| POD1A
    
    style POD1A fill:#e3f2fd
    style POD2A fill:#e8f5e8
    style POD3A fill:#fff3e0
    style MASTER fill:#ffcdd2
```

### Kubernetes ç¶²è·¯éœ€æ±‚

Kubernetes ç¶²è·¯æ¨¡å‹è¦æ±‚ï¼š

1. **æ¯å€‹ Pod éƒ½æœ‰å”¯ä¸€çš„ IP åœ°å€**
2. **Pod é–“å¯ä»¥ç›´æ¥é€šè¨Šï¼ˆç„¡éœ€ NATï¼‰**
3. **Node èˆ‡ Pod é–“å¯ä»¥ç›´æ¥é€šè¨Š**
4. **Pod çœ‹åˆ°çš„è‡ªå·± IP èˆ‡å…¶ä»– Pod çœ‹åˆ°çš„ä¸€è‡´**

---

## ğŸ”Œ CNI (Container Network Interface) æ¦‚å¿µ

### ä»€éº¼æ˜¯ CNIï¼Ÿ

CNI æ˜¯ä¸€å€‹æ¨™æº–åŒ–çš„æ¥å£ï¼Œç”¨æ–¼é…ç½® Linux å®¹å™¨çš„ç¶²è·¯ã€‚å®ƒå®šç¾©äº†ï¼š

- **ç¶²è·¯æ’ä»¶çš„æ¨™æº–æ¥å£**
- **é…ç½®æ ¼å¼è¦ç¯„**
- **åŸ·è¡Œæ™‚è¡Œç‚ºå®šç¾©**

### CNI æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph cluster [Kubernetes Cluster]
        subgraph master [Master Node]
            MASTER[Control Plane<br/>192.168.1.10]
        end
        
        subgraph node1 [Worker Node 1]
            NODE1[kubelet<br/>192.168.1.11]
            POD1A[Pod A<br/>10.244.1.2]
            POD1B[Pod B<br/>10.244.1.3]
        end
        
        subgraph node2 [Worker Node 2]
            NODE2[kubelet<br/>192.168.1.12]
            POD2A[Pod C<br/>10.244.2.2]
            POD2B[Pod D<br/>10.244.2.3]
        end
        
        subgraph node3 [Worker Node 3]
            NODE3[kubelet<br/>192.168.1.13]
            POD3A[Pod E<br/>10.244.3.2]
            POD3B[Pod F<br/>10.244.3.3]
        end
    end
    
    POD1A -.-> POD2A
    POD1B -.-> POD3A
    POD2B -.-> POD1A
    
    style POD1A fill:#e3f2fd
    style POD2A fill:#e8f5e8
    style POD3A fill:#fff3e0
    style MASTER fill:#ffcdd2
```

### CNI å·¥ä½œæµç¨‹

```mermaid
sequenceDiagram
    participant K as kubelet
    participant C as Container Runtime
    participant CNI as CNI Plugin
    participant N as Network Stack
    
    K->>C: Create Pod
    C->>CNI: ADD command
    CNI->>N: Create veth pair
    CNI->>N: Configure IP address
    CNI->>N: Setup routing
    CNI->>N: Configure iptables
    CNI-->>C: Return network config
    C-->>K: Pod creation complete
    
    Note over K,N: Pod is running...
    
    K->>C: Delete Pod
    C->>CNI: DEL command
    CNI->>N: Clean network resources
    CNI-->>C: Cleanup complete
    C-->>K: Pod deletion complete
```

---

## ğŸŒŠ Flannel CNI æ·±å…¥è§£æ

### Flannel æ˜¯ä»€éº¼ï¼Ÿ

Flannel æ˜¯ CoreOS é–‹ç™¼çš„ç°¡å–®æ˜“ç”¨çš„ CNI æ’ä»¶ï¼Œå°ˆç‚º Kubernetes è¨­è¨ˆã€‚

**è¨­è¨ˆç†å¿µ**ï¼š
- ç°¡å–®æ€§å„ªæ–¼è¤‡é›œæ€§
- ç‚ºæ¯å€‹ç¯€é»åˆ†é…ä¸€å€‹å­ç¶²æ®µ
- é€šéå°è£æŠ€è¡“å¯¦ç¾è·¨ç¯€é»é€šè¨Š

### Flannel æ ¸å¿ƒæ¦‚å¿µ

```mermaid
graph TB
    subgraph cluster ["Flannel æ¦‚å¿µæ¶æ§‹"]
        subgraph network ["é›†ç¾¤ç¶²è·¯"]
            CLUSTER_CIDR["Cluster CIDR<br/>10.244.0.0/16"]
        end
        
        subgraph subnets ["ç¯€é»å­ç¶²åˆ†é…"]
            NODE1_SUBNET["Node 1 Subnet<br/>10.244.1.0/24"]
            NODE2_SUBNET["Node 2 Subnet<br/>10.244.2.0/24"]
            NODE3_SUBNET["Node 3 Subnet<br/>10.244.3.0/24"]
        end
        
        subgraph backends ["å¾Œç«¯æ¨¡å¼"]
            VXLAN["VXLAN å°è£"]
            HOST_GW["host-gw è·¯ç”±"]
            UDP["UDP å°è£"]
        end
    end
    
    CLUSTER_CIDR --> NODE1_SUBNET
    CLUSTER_CIDR --> NODE2_SUBNET
    CLUSTER_CIDR --> NODE3_SUBNET
    
    NODE1_SUBNET --> VXLAN
    NODE2_SUBNET --> HOST_GW
    NODE3_SUBNET --> UDP
    
    style CLUSTER_CIDR fill:#e3f2fd
    style NODE1_SUBNET fill:#e8f5e8
    style NODE2_SUBNET fill:#fff3e0
    style NODE3_SUBNET fill:#f3e5f5
```

### Flannel çµ„ä»¶æ¶æ§‹

```mermaid
graph TB
    subgraph node ["æ¯å€‹ç¯€é»ä¸Šçš„ Flannel çµ„ä»¶"]
        subgraph daemon ["flanneld DaemonSet"]
            FLANNELD["flanneld é€²ç¨‹"]
            CONFIG["é…ç½®ç®¡ç†"]
            SUBNET["å­ç¶²åˆ†é…"]
            BACKEND["å¾Œç«¯ç®¡ç†"]
        end
        
        subgraph interfaces ["ç¶²è·¯ä»‹é¢"]
            FLANNEL_IFACE["flannel.1<br/>(VXLAN ä»‹é¢)"]
            CNI0["cni0 bridge"]
            VETH_PAIRS["veth pairs"]
        end
        
        subgraph storage ["è³‡æ–™å­˜å„²"]
            ETCD["etcd/k8s API"]
            SUBNET_DB["å­ç¶²è³‡æ–™åº«"]
        end
    end
    
    FLANNELD --> CONFIG
    FLANNELD --> SUBNET
    FLANNELD --> BACKEND
    
    BACKEND --> FLANNEL_IFACE
    SUBNET --> CNI0
    CNI0 --> VETH_PAIRS
    
    CONFIG --> ETCD
    SUBNET --> SUBNET_DB
    
    style FLANNELD fill:#e3f2fd
    style FLANNEL_IFACE fill:#e8f5e8
    style CNI0 fill:#fff3e0
```

---

## ğŸ“¦ Flannel å¾Œç«¯æ¨¡å¼è©³è§£

### 1. VXLAN æ¨¡å¼ï¼ˆé è¨­ï¼‰

**ç‰¹é»**ï¼šä½¿ç”¨ VXLAN éš§é“æŠ€è¡“ï¼Œåœ¨ UDP å°åŒ…ä¸­å°è£åŸå§‹çš„ä¹™å¤ªç¶²è·¯å¹€ã€‚

```mermaid
graph TB
    subgraph node1 ["Node 1 (192.168.1.11)"]
        POD1["Pod A<br/>10.244.1.2"]
        CNI0_1["cni0<br/>10.244.1.1"]
        FLANNEL1["flannel.1<br/>VXLAN ä»‹é¢"]
        ETH1["eth0<br/>192.168.1.11"]
    end
    
    subgraph node2 ["Node 2 (192.168.1.12)"]
        POD2["Pod B<br/>10.244.2.2"]
        CNI0_2["cni0<br/>10.244.2.1"]
        FLANNEL2["flannel.1<br/>VXLAN ä»‹é¢"]
        ETH2["eth0<br/>192.168.1.12"]
    end
    
    subgraph packet ["ç¶²è·¯å°åŒ…"]
        INNER["å…§å±¤å°åŒ…<br/>Src: 10.244.1.2<br/>Dst: 10.244.2.2"]
        VXLAN_HDR["VXLAN Header<br/>VNI: 1"]
        UDP_HDR["UDP Header<br/>Port: 8472"]
        OUTER["å¤–å±¤å°åŒ…<br/>Src: 192.168.1.11<br/>Dst: 192.168.1.12"]
    end
    
    POD1 --> CNI0_1
    CNI0_1 --> FLANNEL1
    FLANNEL1 --> ETH1
    
    ETH2 --> FLANNEL2
    FLANNEL2 --> CNI0_2
    CNI0_2 --> POD2
    
    INNER --> VXLAN_HDR
    VXLAN_HDR --> UDP_HDR
    UDP_HDR --> OUTER
    
    ETH1 -.->|VXLAN å°è£| ETH2
    
    style POD1 fill:#e3f2fd
    style POD2 fill:#e8f5e8
    style FLANNEL1 fill:#fff3e0
    style FLANNEL2 fill:#fff3e0
```

**VXLAN å°åŒ…çµæ§‹**ï¼š
```
+------------------+
| å¤–å±¤ Ethernet    |
+------------------+
| å¤–å±¤ IP Header   |
+------------------+
| UDP Header       |
+------------------+
| VXLAN Header     |
+------------------+
| å…§å±¤ Ethernet    |
+------------------+
| å…§å±¤ IP Header   |
+------------------+
| æ‡‰ç”¨è³‡æ–™         |
+------------------+
```

### 2. host-gw æ¨¡å¼

**ç‰¹é»**ï¼šä½¿ç”¨ä¸»æ©Ÿè·¯ç”±è¡¨ï¼Œç›´æ¥è·¯ç”±è€Œä¸å°è£ã€‚

```mermaid
graph TB
    subgraph node1 ["Node 1 (192.168.1.11)"]
        POD1["Pod A<br/>10.244.1.2"]
        CNI0_1["cni0<br/>10.244.1.1"]
        ETH1["eth0<br/>192.168.1.11"]
        ROUTE1["è·¯ç”±è¡¨<br/>10.244.2.0/24 via 192.168.1.12"]
    end
    
    subgraph node2 ["Node 2 (192.168.1.12)"]
        POD2["Pod B<br/>10.244.2.2"]
        CNI0_2["cni0<br/>10.244.2.1"]
        ETH2["eth0<br/>192.168.1.12"]
        ROUTE2["è·¯ç”±è¡¨<br/>10.244.1.0/24 via 192.168.1.11"]
    end
    
    subgraph network ["ç¶²è·¯äº¤æ›æ©Ÿ"]
        SWITCH["Layer 2 Switch"]
    end
    
    POD1 --> CNI0_1
    CNI0_1 --> ROUTE1
    ROUTE1 --> ETH1
    ETH1 --> SWITCH
    SWITCH --> ETH2
    ETH2 --> CNI0_2
    CNI0_2 --> POD2
    
    style POD1 fill:#e3f2fd
    style POD2 fill:#e8f5e8
    style SWITCH fill:#fff3e0
    style ROUTE1 fill:#f3e5f5
    style ROUTE2 fill:#f3e5f5
```

**host-gw è·¯ç”±è¡¨ç¯„ä¾‹**ï¼š
```bash
# Node 1 è·¯ç”±è¡¨
10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1
10.244.2.0/24 via 192.168.1.12 dev eth0
10.244.3.0/24 via 192.168.1.13 dev eth0
```

### 3. UDP æ¨¡å¼ï¼ˆå·²æ£„ç”¨ï¼‰

**ç‰¹é»**ï¼šä½¿ç”¨ç”¨æˆ¶ç©ºé–“çš„ UDP å°è£ï¼Œæ€§èƒ½è¼ƒå·®ã€‚

### å¾Œç«¯æ¨¡å¼æ¯”è¼ƒè¡¨

| ç‰¹æ€§ | VXLAN | host-gw | UDP |
|------|-------|---------|-----|
| **å°è£æ–¹å¼** | å…§æ ¸ VXLAN | ç›´æ¥è·¯ç”± | ç”¨æˆ¶ç©ºé–“ UDP |
| **æ€§èƒ½** | ä¸­ç­‰ | æœ€ä½³ | æœ€å·® |
| **ç¶²è·¯è¦æ±‚** | ä»»æ„ | Layer 2 é€£é€š | ä»»æ„ |
| **MTU å½±éŸ¿** | -50 bytes | ç„¡å½±éŸ¿ | -28 bytes |
| **è¤‡é›œåº¦** | ä¸­ç­‰ | ç°¡å–® | è¤‡é›œ |
| **æ¨è–¦å ´æ™¯** | é€šç”¨å ´æ™¯ | åŒç¶²æ®µç¯€é» | å·²æ£„ç”¨ |

---

## ğŸ”¬ å¯¦ä½œ 1ï¼šå»ºç«‹ä½¿ç”¨ Flannel CNI çš„è‡ªè¨‚ KinD å¢é›†

### æº–å‚™ KinD é…ç½®æ–‡ä»¶

```yaml
# kind-flannel-config.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: flannel-cluster

# ç¶²è·¯é…ç½®
networking:
  # ç¦ç”¨é è¨­ CNIï¼Œæˆ‘å€‘è¦æ‰‹å‹•å®‰è£ Flannel
  disableDefaultCNI: true
  # Pod å­ç¶²ç¯„åœ
  podSubnet: "10.244.0.0/16"
  # Service å­ç¶²ç¯„åœ
  serviceSubnet: "10.96.0.0/12"

# ç¯€é»é…ç½®
nodes:
# Control plane ç¯€é»
- role: control-plane
  image: kindest/node:v1.28.0
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "node-type=control-plane"
  - |
    kind: ClusterConfiguration
    controllerManager:
      extraArgs:
        bind-address: "0.0.0.0"
    scheduler:
      extraArgs:
        bind-address: "0.0.0.0"
    etcd:
      local:
        extraArgs:
          listen-metrics-urls: "http://0.0.0.0:2381"
  
  # ç«¯å£æ˜ å°„ï¼Œæ–¹ä¾¿æ¸¬è©¦
  extraPortMappings:
  - containerPort: 30080
    hostPort: 30080
    protocol: TCP
  - containerPort: 30443
    hostPort: 30443
    protocol: TCP

# Worker ç¯€é» 1
- role: worker
  image: kindest/node:v1.28.0
  kubeadmConfigPatches:
  - |
    kind: JoinConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "node-type=worker,zone=zone-a"

# Worker ç¯€é» 2
- role: worker
  image: kindest/node:v1.28.0
  kubeadmConfigPatches:
  - |
    kind: JoinConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "node-type=worker,zone=zone-b"

# Worker ç¯€é» 3
- role: worker
  image: kindest/node:v1.28.0
  kubeadmConfigPatches:
  - |
    kind: JoinConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "node-type=worker,zone=zone-c"
```

### å»ºç«‹å¢é›†è…³æœ¬

```bash
#!/bin/bash
# setup-flannel-cluster.sh

set -e

echo "ğŸš€ å»ºç«‹ Flannel CNI æ¸¬è©¦å¢é›†"

# 1. æª¢æŸ¥å¿…è¦å·¥å…·
echo "ğŸ” æª¢æŸ¥å¿…è¦å·¥å…·..."

if ! command -v kind &> /dev/null; then
    echo "âŒ KinD æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ KinD"
    exit 1
fi

if ! command -v kubectl &> /dev/null; then
    echo "âŒ kubectl æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ kubectl"
    exit 1
fi

if ! command -v docker &> /dev/null; then
    echo "âŒ Docker æœªå®‰è£ï¼Œè«‹å…ˆå®‰è£ Docker"
    exit 1
fi

echo "âœ… æ‰€æœ‰å¿…è¦å·¥å…·å·²å®‰è£"

# 2. åˆªé™¤ç¾æœ‰å¢é›†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
echo "ğŸ—‘ï¸  æ¸…ç†ç¾æœ‰å¢é›†..."
kind delete cluster --name flannel-cluster 2>/dev/null || true

# 3. å»ºç«‹æ–°å¢é›†
echo "ğŸ—ï¸  å»ºç«‹ KinD å¢é›†..."
kind create cluster --config kind-flannel-config.yaml --wait 300s

# 4. ç­‰å¾…ç¯€é»å°±ç·’ï¼ˆä½† CNI å°šæœªå®‰è£ï¼Œæ‰€ä»¥æœƒæ˜¯ NotReadyï¼‰
echo "â³ ç­‰å¾…ç¯€é»å•Ÿå‹•..."
sleep 30

# 5. æª¢æŸ¥ç¯€é»ç‹€æ…‹
echo "ğŸ“Š æª¢æŸ¥ç¯€é»ç‹€æ…‹ï¼ˆCNI æœªå®‰è£å‰ï¼‰ï¼š"
kubectl get nodes -o wide

echo "ğŸ“‹ æª¢æŸ¥ç¯€é»è©³ç´°è³‡è¨Šï¼š"
kubectl describe nodes | grep -E "(Name:|Taints:|Conditions:)" -A 5

# 6. æª¢æŸ¥ Pod ç‹€æ…‹ï¼ˆæ‡‰è©²éƒ½æ˜¯ Pendingï¼‰
echo "ğŸ“¦ æª¢æŸ¥ç³»çµ± Pod ç‹€æ…‹ï¼ˆCNI æœªå®‰è£å‰ï¼‰ï¼š"
kubectl get pods -A -o wide

# 7. ä¸‹è¼‰ Flannel é…ç½®
echo "ğŸ“¥ ä¸‹è¼‰ Flannel é…ç½®æ–‡ä»¶..."
curl -s https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml > kube-flannel.yml

# 8. ä¿®æ”¹ Flannel é…ç½®ä»¥ç¬¦åˆæˆ‘å€‘çš„ç¶²è·¯è¨­å®š
echo "âš™ï¸  èª¿æ•´ Flannel é…ç½®..."

# å‚™ä»½åŸå§‹é…ç½®
cp kube-flannel.yml kube-flannel-original.yml

# ä¿®æ”¹ç¶²è·¯é…ç½®
sed -i 's|"Network": "10.244.0.0/16"|"Network": "10.244.0.0/16"|g' kube-flannel.yml
sed -i 's|"Backend": {"Type": "vxlan"}|"Backend": {"Type": "vxlan", "Port": 8472}|g' kube-flannel.yml

# 9. å®‰è£ Flannel CNI
echo "ğŸŒ å®‰è£ Flannel CNI..."
kubectl apply -f kube-flannel.yml

# 10. ç­‰å¾… Flannel Pod å°±ç·’
echo "â³ ç­‰å¾… Flannel Pod å°±ç·’..."
kubectl wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s

# 11. ç­‰å¾…æ‰€æœ‰ç¯€é»å°±ç·’
echo "â³ ç­‰å¾…æ‰€æœ‰ç¯€é»å°±ç·’..."
kubectl wait --for=condition=ready nodes --all --timeout=300s

# 12. æª¢æŸ¥æœ€çµ‚ç‹€æ…‹
echo "âœ… å¢é›†å»ºç«‹å®Œæˆï¼"
echo ""
echo "ğŸ“Š æœ€çµ‚ç¯€é»ç‹€æ…‹ï¼š"
kubectl get nodes -o wide

echo ""
echo "ğŸ“¦ ç³»çµ± Pod ç‹€æ…‹ï¼š"
kubectl get pods -A -o wide

echo ""
echo "ğŸŒ Flannel Pod ç‹€æ…‹ï¼š"
kubectl get pods -n kube-flannel -o wide

echo ""
echo "ğŸ” ç¶²è·¯é…ç½®æª¢æŸ¥ï¼š"
echo "Pod CIDR: $(kubectl cluster-info dump | grep -o 'cluster-cidr=[^"]*' | head -1)"
echo "Service CIDR: $(kubectl cluster-info dump | grep -o 'service-cluster-ip-range=[^"]*' | head -1)"

# 13. é¡¯ç¤ºæœ‰ç”¨çš„å‘½ä»¤
echo ""
echo "ğŸ¯ æœ‰ç”¨çš„å‘½ä»¤ï¼š"
echo "æŸ¥çœ‹ç¯€é»ï¼škubectl get nodes -o wide"
echo "æŸ¥çœ‹ Flannel é…ç½®ï¼škubectl get configmap kube-flannel-cfg -n kube-flannel -o yaml"
echo "æŸ¥çœ‹ Flannel æ—¥èªŒï¼škubectl logs -n kube-flannel -l app=flannel"
echo "é€²å…¥ç¯€é»ï¼šdocker exec -it flannel-cluster-control-plane bash"
echo "åˆªé™¤å¢é›†ï¼škind delete cluster --name flannel-cluster"

echo ""
echo "ğŸ‰ Flannel CNI å¢é›†å»ºç«‹å®Œæˆï¼"
```

### åŸ·è¡Œå»ºç«‹å¢é›†

```bash
# çµ¦è…³æœ¬åŸ·è¡Œæ¬Šé™
chmod +x setup-flannel-cluster.sh

# åŸ·è¡Œè…³æœ¬
./setup-flannel-cluster.sh
```

---

        cat > /usr/share/nginx/html/index.html << EOF
        <!DOCTYPE html>
        <html>
        <head>
            <title>Network Test - Zone B</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; background: #e8f5e8; }
                .info { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
                .highlight { color: #388e3c; font-weight: bold; }
            </style>
        </head>
        <body>
            <div class="info">
                <h1>ğŸŒ Network Test Pod - Zone B</h1>
                <p><strong>Pod Name:</strong> <span class="highlight">\$POD_NAME</span></p>
                <p><strong>Node Name:</strong> <span class="highlight">\$NODE_NAME</span></p>
                <p><strong>Pod IP:</strong> <span class="highlight">\$POD_IP</span></p>
                <p><strong>Zone:</strong> <span class="highlight">zone-b</span></p>
                <p><strong>Timestamp:</strong> <span class="highlight">\$(date)</span></p>
                <hr>
                <h3>Network Information:</h3>
                <pre>\$(ip addr show eth0)</pre>
                <hr>
                <h3>Routing Table:</h3>
                <pre>\$(ip route)</pre>
            </div>
        </body>
        </html>
        EOF
        
        nginx -g "daemon off;"
      resources:
        requests:
          memory: "64Mi"
          cpu: "50m"
        limits:
          memory: "128Mi"
          cpu: "100m"

---
apiVersion: apps/v1
kind: Deployment
metadata:
name: test-app-zone-c
namespace: network-test
labels:
  app: test-app
  zone: zone-c
spec:
replicas: 2
selector:
  matchLabels:
    app: test-app
    zone: zone-c
template:
  metadata:
    labels:
      app: test-app
      zone: zone-c
  spec:
    nodeSelector:
      zone: zone-c
    containers:
    - name: test-container
      image: nginx:1.21-alpine
      ports:
      - containerPort: 80
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      command: ["/bin/sh", "-c"]
      args:
      - |
        cat > /usr/share/nginx/html/index.html << EOF
        <!DOCTYPE html>
        <html>
        <head>
            <title>Network Test - Zone C</title>
            <style>
                body { font-family: Arial, sans-serif; margin: 40px; background: #fff3e0; }
                .info { background: white; padding: 20px; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }
                .highlight { color: #f57c00; font-weight: bold; }
            </style>
        </head>
        <body>
            <div class="info">
                <h1>ğŸŒ Network Test Pod - Zone C</h1>
                <p><strong>Pod Name:</strong> <span class="highlight">\$POD_NAME</span></p>
                <p><strong>Node Name:</strong> <span class="highlight">\$NODE_NAME</span></p>
                <p><strong>Pod IP:</strong> <span class="highlight">\$POD_IP</span></p>
                <p><strong>Zone:</strong> <span class="highlight">zone-c</span></p>
                <p><strong>Timestamp:</strong> <span class="highlight">\$(date)</span></p>
                <hr>
                <h3>Network Information:</h3>
                <pre>\$(ip addr show eth0)</pre>
                <hr>
                <h3>Routing Table:</h3>
                <pre>\$(ip route)</pre>
            </div>
        </body>
        </html>
        EOF
        
        nginx -g "daemon off;"
      resources:
        requests:
          memory: "64Mi"
          cpu: "50m"
        limits:
          memory: "128Mi"
          cpu: "100m"

---
# ç¶²è·¯æ¸¬è©¦å·¥å…· Pod
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
name: network-debug-tools
namespace: network-test
labels:
  app: network-debug
spec:
replicas: 1
selector:
  matchLabels:
    app: network-debug
template:
  metadata:
    labels:
      app: network-debug
  spec:
    containers:
    - name: debug-tools
      image: nicolaka/netshoot:latest
      command: ["/bin/bash", "-c", "sleep infinity"]
      env:
      - name: NODE_NAME
        valueFrom:
          fieldRef:
            fieldPath: spec.nodeName
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_IP
        valueFrom:
          fieldRef:
            fieldPath: status.podIP
      resources:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "256Mi"
          cpu: "200m"
```

---
# ç‚ºæ¸¬è©¦æ‡‰ç”¨å‰µå»º Service
```yaml
apiVersion: v1
kind: Service
metadata:
name: test-app-zone-a-service
namespace: network-test
labels:
  zone: zone-a
spec:
selector:
  app: test-app
  zone: zone-a
ports:
- port: 80
  targetPort: 80
  name: http
type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
name: test-app-zone-b-service
namespace: network-test
labels:
  zone: zone-b
spec:
selector:
  app: test-app
  zone: zone-b
ports:
- port: 80
  targetPort: 80
  name: http
type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
name: test-app-zone-c-service
namespace: network-test
labels:
  zone: zone-c
spec:
selector:
  app: test-app
  zone: zone-c
ports:
- port: 80
  targetPort: 80
  name: http
type: ClusterIP

---
# æ¸¬è©¦ç”¨çš„ NodePort Service
apiVersion: v1
kind: Service
metadata:
name: test-app-nodeport
namespace: network-test
labels:
  service-type: nodeport
spec:
selector:
  app: test-app
ports:
- port: 80
  targetPort: 80
  nodePort: 30080
  name: http
type: NodePort
```

### ç¶²è·¯é€šè¨Šæ¸¬è©¦è…³æœ¬

```bash
#!/bin/bash
# test-pod-networking.sh

set -e

echo "ğŸ§ª é–‹å§‹ Pod é–“ç¶²è·¯é€šè¨Šæ¸¬è©¦"

# 1. éƒ¨ç½²æ¸¬è©¦æ‡‰ç”¨
echo "ğŸ“¦ éƒ¨ç½²æ¸¬è©¦æ‡‰ç”¨..."
kubectl apply -f test-network-apps.yaml

# 2. ç­‰å¾…æ‰€æœ‰ Pod å°±ç·’
echo "â³ ç­‰å¾… Pod å°±ç·’..."
kubectl wait --for=condition=ready pod -l app=test-app -n network-test --timeout=300s
kubectl wait --for=condition=ready pod -l app=network-debug -n network-test --timeout=300s

# 3. ç²å– Pod è³‡è¨Š
echo "ğŸ“‹ ç²å– Pod è³‡è¨Š..."
echo ""
echo "=== æ‰€æœ‰æ¸¬è©¦ Pod ==="
kubectl get pods -n network-test -o wide

echo ""
echo "=== ç¯€é»åˆ†ä½ˆæƒ…æ³ ==="
kubectl get pods -n network-test -o custom-columns="NAME:.metadata.name,NODE:.spec.nodeName,POD_IP:.status.podIP,ZONE:.metadata.labels.zone"

# 4. ç²å– Service è³‡è¨Š
echo ""
echo "=== Service è³‡è¨Š ==="
kubectl get services -n network-test -o wide

# 5. ç²å–ç¶²è·¯èª¿è©¦å·¥å…· Pod
DEBUG_POD=$(kubectl get pods -n network-test -l app=network-debug -o jsonpath='{.items[0].metadata.name}')
echo ""
echo "ğŸ” ç¶²è·¯èª¿è©¦å·¥å…· Pod: $DEBUG_POD"

# 6. æ¸¬è©¦åŒç¯€é» Pod é–“é€šè¨Š
echo ""
echo "ğŸ”„ æ¸¬è©¦åŒç¯€é» Pod é–“é€šè¨Š..."

# ç²å–åŒä¸€ç¯€é»ä¸Šçš„å…©å€‹ Pod
ZONE_A_PODS=($(kubectl get pods -n network-test -l zone=zone-a -o jsonpath='{.items[*].status.podIP}'))
if [ ${#ZONE_A_PODS[@]} -ge 2 ]; then
  echo "æ¸¬è©¦ Zone A å…§éƒ¨é€šè¨Š: ${ZONE_A_PODS[0]} -> ${ZONE_A_PODS[1]}"
  kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://${ZONE_A_PODS[1]} | grep -o '<h1>.*</h1>' || echo "âŒ é€£ç·šå¤±æ•—"
fi

# 7. æ¸¬è©¦è·¨ç¯€é» Pod é–“é€šè¨Š
echo ""
echo "ğŸŒ æ¸¬è©¦è·¨ç¯€é» Pod é–“é€šè¨Š..."

# ç²å–ä¸åŒç¯€é»çš„ Pod IP
ZONE_A_IP=$(kubectl get pods -n network-test -l zone=zone-a -o jsonpath='{.items[0].status.podIP}')
ZONE_B_IP=$(kubectl get pods -n network-test -l zone=zone-b -o jsonpath='{.items[0].status.podIP}')
ZONE_C_IP=$(kubectl get pods -n network-test -l zone=zone-c -o jsonpath='{.items[0].status.podIP}')

echo "Zone A Pod IP: $ZONE_A_IP"
echo "Zone B Pod IP: $ZONE_B_IP"
echo "Zone C Pod IP: $ZONE_C_IP"

echo ""
echo "æ¸¬è©¦ Zone A -> Zone B é€šè¨Š:"
kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://$ZONE_B_IP | grep -o '<h1>.*</h1>' || echo "âŒ é€£ç·šå¤±æ•—"

echo ""
echo "æ¸¬è©¦ Zone A -> Zone C é€šè¨Š:"
kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://$ZONE_C_IP | grep -o '<h1>.*</h1>' || echo "âŒ é€£ç·šå¤±æ•—"

echo ""
echo "æ¸¬è©¦ Zone B -> Zone C é€šè¨Š:"
kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://$ZONE_C_IP | grep -o '<h1>.*</h1>' || echo "âŒ é€£ç·šå¤±æ•—"

# 8. æ¸¬è©¦ Service é€šè¨Š
echo ""
echo "ğŸ”— æ¸¬è©¦ Service é€šè¨Š..."

echo "æ¸¬è©¦ Zone A Service:"
kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://test-app-zone-a-service.network-test.svc.cluster.local | grep -o '<h1>.*</h1>' || echo "âŒ Service é€£ç·šå¤±æ•—"

echo ""
echo "æ¸¬è©¦ Zone B Service:"
kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://test-app-zone-b-service.network-test.svc.cluster.local | grep -o '<h1>.*</h1>' || echo "âŒ Service é€£ç·šå¤±æ•—"

echo ""
echo "æ¸¬è©¦ Zone C Service:"
kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://test-app-zone-c-service.network-test.svc.cluster.local | grep -o '<h1>.*</h1>' || echo "âŒ Service é€£ç·šå¤±æ•—"

# 9. æ¸¬è©¦ DNS è§£æ
echo ""
echo "ğŸ” æ¸¬è©¦ DNS è§£æ..."

echo "è§£æ Zone A Service:"
kubectl exec -n network-test $DEBUG_POD -- nslookup test-app-zone-a-service.network-test.svc.cluster.local

echo ""
echo "è§£æ Zone B Service:"
kubectl exec -n network-test $DEBUG_POD -- nslookup test-app-zone-b-service.network-test.svc.cluster.local

# 10. æ¸¬è©¦ NodePort æœå‹™
echo ""
echo "ğŸŒ æ¸¬è©¦ NodePort æœå‹™..."

# ç²å–ç¯€é» IP
NODE_IPS=($(kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'))
echo "ç¯€é» IP åˆ—è¡¨: ${NODE_IPS[*]}"

for NODE_IP in "${NODE_IPS[@]}"; do
  echo "æ¸¬è©¦ç¯€é» $NODE_IP:30080"
  kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://$NODE_IP:30080 | grep -o '<h1>.*</h1>' || echo "âŒ NodePort é€£ç·šå¤±æ•—"
done

# 11. ç¶²è·¯æ•ˆèƒ½æ¸¬è©¦
echo ""
echo "âš¡ ç¶²è·¯æ•ˆèƒ½æ¸¬è©¦..."

echo "æ¸¬è©¦è·¨ç¯€é»å»¶é² (Zone A -> Zone B):"
kubectl exec -n network-test $DEBUG_POD -- ping -c 3 $ZONE_B_IP

echo ""
echo "æ¸¬è©¦è·¨ç¯€é»é »å¯¬ (ä½¿ç”¨ iperf3):"
# åœ¨ Zone B å•Ÿå‹• iperf3 æœå‹™å™¨
ZONE_B_POD=$(kubectl get pods -n network-test -l zone=zone-b -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n network-test $ZONE_B_POD -- sh -c "iperf3 -s -D" &

sleep 2

# å¾èª¿è©¦å·¥å…·é€£æ¥æ¸¬è©¦
kubectl exec -n network-test $DEBUG_POD -- iperf3 -c $ZONE_B_IP -t 10 -f M || echo "âŒ iperf3 æ¸¬è©¦å¤±æ•—"

echo ""
echo "âœ… ç¶²è·¯é€šè¨Šæ¸¬è©¦å®Œæˆï¼"
```

### åŸ·è¡Œæ¸¬è©¦

```bash
# çµ¦è…³æœ¬åŸ·è¡Œæ¬Šé™
chmod +x test-pod-networking.sh

# åŸ·è¡Œæ¸¬è©¦
./test-pod-networking.sh
```

---

## ğŸ”¬ å¯¦ä½œ 3ï¼šåˆ†æ Flannel ç¶²è·¯è¡Œç‚º

### Flannel ç¶²è·¯åˆ†æè…³æœ¬

```bash
#!/bin/bash
# analyze-flannel-network.sh

set -e

echo "ğŸ”¬ åˆ†æ Flannel ç¶²è·¯è¡Œç‚º"

# 1. æª¢æŸ¥ Flannel é…ç½®
echo "âš™ï¸  æª¢æŸ¥ Flannel é…ç½®..."
echo ""
echo "=== Flannel ConfigMap ==="
kubectl get configmap kube-flannel-cfg -n kube-flannel -o yaml

echo ""
echo "=== Flannel DaemonSet ==="
kubectl get daemonset kube-flannel-ds -n kube-flannel -o wide

echo ""
echo "=== Flannel Pod ç‹€æ…‹ ==="
kubectl get pods -n kube-flannel -o wide

# 2. æª¢æŸ¥ç¯€é»ç¶²è·¯é…ç½®
echo ""
echo "ğŸŒ æª¢æŸ¥ç¯€é»ç¶²è·¯é…ç½®..."

NODES=($(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'))

for NODE in "${NODES[@]}"; do
  echo ""
  echo "=== ç¯€é»: $NODE ==="
  
  echo "Node IP å’Œå­ç¶²åˆ†é…:"
  kubectl get node $NODE -o jsonpath='{.status.addresses[?(@.type=="InternalIP")].address}' && echo ""
  kubectl get node $NODE -o jsonpath='{.spec.podCIDR}' && echo ""
  
  echo ""
  echo "é€²å…¥ç¯€é»æª¢æŸ¥ç¶²è·¯ä»‹é¢:"
  docker exec $NODE ip addr show | grep -E "(flannel|cni|docker)" || true
  
  echo ""
  echo "æª¢æŸ¥è·¯ç”±è¡¨:"
  docker exec $NODE ip route | grep -E "(10\.244|flannel)" || true
  
  echo ""
  echo "æª¢æŸ¥ VXLAN ä»‹é¢:"
  docker exec $NODE ip -d link show flannel.1 2>/dev/null || echo "VXLAN ä»‹é¢ä¸å­˜åœ¨"
  
  echo ""
  echo "æª¢æŸ¥ ARP è¡¨:"
  docker exec $NODE ip neigh show dev flannel.1 2>/dev/null || echo "ç„¡ VXLAN ARP æ¢ç›®"
done

# 3. åˆ†æ Flannel å¾Œç«¯é…ç½®
echo ""
echo "ğŸ” åˆ†æ Flannel å¾Œç«¯é…ç½®..."

# ç²å– Flannel Pod
FLANNEL_POD=$(kubectl get pods -n kube-flannel -l app=flannel -o jsonpath='{.items[0].metadata.name}')

echo "Flannel Pod: $FLANNEL_POD"
echo ""
echo "Flannel æ—¥èªŒ (æœ€è¿‘ 50 è¡Œ):"
kubectl logs -n kube-flannel $FLANNEL_POD --tail=50

echo ""
echo "æª¢æŸ¥ Flannel å­ç¶²åˆ†é…:"
kubectl exec -n kube-flannel $FLANNEL_POD -- cat /run/flannel/subnet.env

# 4. å°åŒ…è¿½è¹¤åˆ†æ
echo ""
echo "ğŸ“¦ å°åŒ…è¿½è¹¤åˆ†æ..."

# ç²å–æ¸¬è©¦ Pod è³‡è¨Š
if kubectl get namespace network-test &>/dev/null; then
  ZONE_A_POD=$(kubectl get pods -n network-test -l zone=zone-a -o jsonpath='{.items[0].metadata.name}')
  ZONE_B_POD=$(kubectl get pods -n network-test -l zone=zone-b -o jsonpath='{.items[0].metadata.name}')
  ZONE_A_IP=$(kubectl get pods -n network-test -l zone=zone-a -o jsonpath='{.items[0].status.podIP}')
  ZONE_B_IP=$(kubectl get pods -n network-test -l zone=zone-b -o jsonpath='{.items[0].status.podIP}')
  
  echo "Zone A Pod: $ZONE_A_POD ($ZONE_A_IP)"
  echo "Zone B Pod: $ZONE_B_POD ($ZONE_B_IP)"
  
  # æª¢æŸ¥ Pod ç¶²è·¯é…ç½®
  echo ""
  echo "Zone A Pod ç¶²è·¯é…ç½®:"
  kubectl exec -n network-test $ZONE_A_POD -- ip addr show eth0
  kubectl exec -n network-test $ZONE_A_POD -- ip route
  
  echo ""
  echo "Zone B Pod ç¶²è·¯é…ç½®:"
  kubectl exec -n network-test $ZONE_B_POD -- ip addr show eth0
  kubectl exec -n network-test $ZONE_B_POD -- ip route
  
  # è¿½è¹¤å°åŒ…è·¯å¾‘
  echo ""
  echo "è¿½è¹¤å°åŒ…è·¯å¾‘ (Zone A -> Zone B):"
  kubectl exec -n network-test $ZONE_A_POD -- traceroute -n $ZONE_B_IP || echo "traceroute ä¸å¯ç”¨"
fi

# 5. æª¢æŸ¥ iptables è¦å‰‡
echo ""
echo "ğŸ”¥ æª¢æŸ¥ iptables è¦å‰‡..."

for NODE in "${NODES[@]}"; do
  echo ""
  echo "=== ç¯€é» $NODE iptables è¦å‰‡ ==="
  
  echo "NAT è¡¨ (POSTROUTING):"
  docker exec $NODE iptables -t nat -L POSTROUTING -n | grep -E "(10\.244|flannel)" || echo "ç„¡ç›¸é—œè¦å‰‡"
  
  echo ""
  echo "Filter è¡¨ (FORWARD):"
  docker exec $NODE iptables -t filter -L FORWARD -n | grep -E "(10\.244|flannel)" || echo "ç„¡ç›¸é—œè¦å‰‡"
done

# 6. æª¢æŸ¥ CNI é…ç½®
echo ""
echo "ğŸ”Œ æª¢æŸ¥ CNI é…ç½®..."

for NODE in "${NODES[@]}"; do
  echo ""
  echo "=== ç¯€é» $NODE CNI é…ç½® ==="
  
  echo "CNI é…ç½®æ–‡ä»¶:"
  docker exec $NODE ls -la /etc/cni/net.d/ || echo "CNI é…ç½®ç›®éŒ„ä¸å­˜åœ¨"
  
  echo ""
  echo "Flannel CNI é…ç½®:"
  docker exec $NODE cat /etc/cni/net.d/10-flannel.conflist 2>/dev/null || echo "Flannel CNI é…ç½®ä¸å­˜åœ¨"
done

# 7. ç¶²è·¯æ€§èƒ½åˆ†æ
echo ""
echo "âš¡ ç¶²è·¯æ€§èƒ½åˆ†æ..."

if kubectl get namespace network-test &>/dev/null; then
  DEBUG_POD=$(kubectl get pods -n network-test -l app=network-debug -o jsonpath='{.items[0].metadata.name}')
  
  echo "æ¸¬è©¦åŒç¯€é»é€šè¨Šå»¶é²:"
  kubectl exec -n network-test $DEBUG_POD -- ping -c 5 $ZONE_A_IP | tail -1
  
  echo ""
  echo "æ¸¬è©¦è·¨ç¯€é»é€šè¨Šå»¶é²:"
  kubectl exec -n network-test $DEBUG_POD -- ping -c 5 $ZONE_B_IP | tail -1
  
  echo ""
  echo "æª¢æŸ¥ MTU è¨­å®š:"
  kubectl exec -n network-test $DEBUG_POD -- ip link show eth0 | grep mtu
fi

# 8. Flannel æ•…éšœæ’é™¤è³‡è¨Š
echo ""
echo "ğŸ©º Flannel æ•…éšœæ’é™¤è³‡è¨Š..."

echo "æª¢æŸ¥ Flannel å¥åº·ç‹€æ…‹:"
kubectl get pods -n kube-flannel -o wide

echo ""
echo "æª¢æŸ¥ç¯€é»å°±ç·’ç‹€æ…‹:"
kubectl get nodes -o wide

echo ""
echo "æª¢æŸ¥ç³»çµ± Pod ç¶²è·¯ç‹€æ…‹:"
kubectl get pods -A -o wide | grep -E "(Pending|Error|CrashLoopBackOff)" || echo "æ‰€æœ‰ Pod ç‹€æ…‹æ­£å¸¸"

echo ""
echo "âœ… Flannel ç¶²è·¯åˆ†æå®Œæˆï¼"

# 9. ç”Ÿæˆç¶²è·¯æ‹“æ’²åœ–è³‡è¨Š
echo ""
echo "ğŸ—ºï¸  ç¶²è·¯æ‹“æ’²è³‡è¨Š..."

echo "ç¯€é»å’Œ Pod åˆ†ä½ˆ:"
kubectl get pods -A -o custom-columns="NAMESPACE:.metadata.namespace,NAME:.metadata.name,NODE:.spec.nodeName,POD_IP:.status.podIP" | grep -v "kube-system"

echo ""
echo "Service å’Œç«¯é»:"
kubectl get endpoints -A -o wide | grep -v "kube-system"
```

### ç¶²è·¯æ‹“æ’²è¦–è¦ºåŒ–

```bash
#!/bin/bash
# generate-network-topology.sh

echo "ğŸ—ºï¸  ç”Ÿæˆ Flannel ç¶²è·¯æ‹“æ’²åœ–"

# 1. æ”¶é›†ç¶²è·¯è³‡è¨Š
echo "ğŸ“Š æ”¶é›†ç¶²è·¯è³‡è¨Š..."

# å‰µå»ºè¼¸å‡ºç›®éŒ„
mkdir -p network-analysis

# æ”¶é›†ç¯€é»è³‡è¨Š
kubectl get nodes -o json > network-analysis/nodes.json
kubectl get pods -A -o json > network-analysis/pods.json
kubectl get services -A -o json > network-analysis/services.json

# 2. ç”Ÿæˆæ‹“æ’²è³‡è¨Š
cat > network-analysis/topology-info.txt << EOF
# Flannel ç¶²è·¯æ‹“æ’²åˆ†æå ±å‘Š

## å¢é›†åŸºæœ¬è³‡è¨Š
$(kubectl cluster-info)

## ç¯€é»è³‡è¨Š
$(kubectl get nodes -o wide)

## Flannel é…ç½®
$(kubectl get configmap kube-flannel-cfg -n kube-flannel -o yaml)

## å­ç¶²åˆ†é…
EOF

# ç²å–æ¯å€‹ç¯€é»çš„å­ç¶²åˆ†é…
NODES=($(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'))
for NODE in "${NODES[@]}"; do
  echo "ç¯€é» $NODE:" >> network-analysis/topology-info.txt
  kubectl get node $NODE -o jsonpath='{.spec.podCIDR}' >> network-analysis/topology-info.txt
  echo "" >> network-analysis/topology-info.txt
done

# 3. ç”Ÿæˆ Mermaid åœ–è¡¨
cat > network-analysis/network-topology.md << 'EOF'
# Flannel ç¶²è·¯æ‹“æ’²åœ–
```

```mermaid
graph TB
  subgraph "Kubernetes Cluster with Flannel CNI"
      subgraph "Control Plane"
          CP[Control Plane Node<br/>flannel-cluster-control-plane]
      end
      
      subgraph "Worker Nodes"
          subgraph "Worker Node 1"
              WN1[flannel-cluster-worker<br/>Zone A]
              WN1_SUBNET[Pod Subnet: 10.244.1.0/24]
              WN1_FLANNEL[flannel.1 VXLAN Interface]
              WN1_CNI[cni0 Bridge]
              
              subgraph "Pods on Node 1"
                  POD1A[test-app-zone-a-xxx<br/>10.244.1.2]
                  POD1B[test-app-zone-a-yyy<br/>10.244.1.3]
              end
          end
          
          subgraph "Worker Node 2"
              WN2[flannel-cluster-worker2<br/>Zone B]
              WN2_SUBNET[Pod Subnet: 10.244.2.0/24]
              WN2_FLANNEL[flannel.1 VXLAN Interface]
              WN2_CNI[cni0 Bridge]
              
              subgraph "Pods on Node 2"
                  POD2A[test-app-zone-b-xxx<br/>10.244.2.2]
                  POD2B[test-app-zone-b-yyy<br/>10.244.2.3]
              end
          end
          
          subgraph "Worker Node 3"
              WN3[flannel-cluster-worker3<br/>Zone C]
              WN3_SUBNET[Pod Subnet: 10.244.3.0/24]
              WN3_FLANNEL[flannel.1 VXLAN Interface]
              WN3_CNI[cni0 Bridge]
              
              subgraph "Pods on Node 3"
                  POD3A[test-app-zone-c-xxx<br/>10.244.3.2]
                  POD3B[network-debug-tools<br/>10.244.3.3]
              end
          end
      end
      
      subgraph "Flannel DaemonSet"
          FLANNEL_DS[kube-flannel-ds]
          FLANNEL_POD1[flannel-pod-1]
          FLANNEL_POD2[flannel-pod-2]
          FLANNEL_POD3[flannel-pod-3]
      end
      
      subgraph "Services"
          SVC_A[test-app-zone-a-service<br/>ClusterIP]
          SVC_B[test-app-zone-b-service<br/>ClusterIP]
          SVC_C[test-app-zone-c-service<br/>ClusterIP]
          SVC_NP[test-app-nodeport<br/>NodePort:30080]
      end
  end
  
  subgraph "Network Flow"
      VXLAN_TUNNEL[VXLAN Tunnel<br/>UDP Port 8472]
  end
  
  %% é€£æ¥é—œä¿‚
  POD1A --> WN1_CNI
  POD1B --> WN1_CNI
  WN1_CNI --> WN1_FLANNEL
  
  POD2A --> WN2_CNI
  POD2B --> WN2_CNI
  WN2_CNI --> WN2_FLANNEL
  
  POD3A --> WN3_CNI
  POD3B --> WN3_CNI
  WN3_CNI --> WN3_FLANNEL
  
  %% VXLAN éš§é“
  WN1_FLANNEL -.->|VXLAN| VXLAN_TUNNEL
  WN2_FLANNEL -.->|VXLAN| VXLAN_TUNNEL
  WN3_FLANNEL -.->|VXLAN| VXLAN_TUNNEL
  
  %% Service é€£æ¥
  SVC_A --> POD1A
  SVC_A --> POD1B
  SVC_B --> POD2A
  SVC_B --> POD2B
  SVC_C --> POD3A
  
  SVC_NP --> POD1A
  SVC_NP --> POD1B
  SVC_NP --> POD2A
  SVC_NP --> POD2B
  SVC_NP --> POD3A
  
  %% Flannel DaemonSet
  FLANNEL_DS --> FLANNEL_POD1
  FLANNEL_DS --> FLANNEL_POD2
  FLANNEL_DS --> FLANNEL_POD3
  
  FLANNEL_POD1 --> WN1_FLANNEL
  FLANNEL_POD2 --> WN2_FLANNEL
  FLANNEL_POD3 --> WN3_FLANNEL
  
%% æ¨£å¼
style CP fill:#ffcdd2
style WN1 fill:#e3f2fd
style WN2 fill:#e8f5e8
style WN3 fill:#fff3e0
style POD1A fill:#bbdefb
style POD1B fill:#bbdefb
style POD2A fill:#c8e6c9
style POD2B fill:#c8e6c9
style POD3A fill:#ffe0b2
style POD3B fill:#ffe0b2
style VXLAN_TUNNEL fill:#f3e5f5
```


# 4. ç”Ÿæˆè©³ç´°çš„ç¶²è·¯åˆ†æå ±å‘Š
cat > network-analysis/detailed-analysis.md << 'EOF'
# Flannel CNI è©³ç´°ç¶²è·¯åˆ†æå ±å‘Š

## 1. ç¶²è·¯æ¶æ§‹æ¦‚è¦½

### å¢é›†ç¶²è·¯é…ç½®
- **Pod CIDR**: 10.244.0.0/16
- **Service CIDR**: 10.96.0.0/12
- **CNI Plugin**: Flannel
- **Backend Mode**: VXLAN (é è¨­)

### ç¯€é»å­ç¶²åˆ†é…


# æ·»åŠ å¯¦éš›çš„ç¯€é»å­ç¶²è³‡è¨Š
```
for NODE in "${NODES[@]}"; do
  SUBNET=$(kubectl get node $NODE -o jsonpath='{.spec.podCIDR}')
  echo "- **$NODE**: $SUBNET" >> network-analysis/detailed-analysis.md
done

cat >> network-analysis/detailed-analysis.md << 'EOF'
```

## 2. Flannel VXLAN å·¥ä½œåŸç†

### VXLAN å°è£æµç¨‹

1. **Pod A (10.244.1.2)** ç™¼é€å°åŒ…åˆ° **Pod B (10.244.2.2)**
2. å°åŒ…é€šé **veth pair** åˆ°é” **cni0 bridge**
3. **cni0** æ ¹æ“šè·¯ç”±è¡¨è½‰ç™¼åˆ° **flannel.1** VXLAN ä»‹é¢
4. **flannel.1** å°‡åŸå§‹å°åŒ…å°è£åœ¨ VXLAN header ä¸­
5. å¤–å±¤ä½¿ç”¨ç¯€é» IP é€²è¡Œè·¯ç”± (192.168.x.x)
6. ç›®æ¨™ç¯€é»çš„ **flannel.1** è§£å°è£ä¸¦è½‰ç™¼åˆ°ç›®æ¨™ Pod

### å°åŒ…çµæ§‹åˆ†æ

```
åŸå§‹å°åŒ…: [Eth][IP: 10.244.1.2 -> 10.244.2.2][TCP][Data]
                   â†“ VXLAN å°è£
VXLANå°åŒ…: [Eth][IP: 192.168.1.11 -> 192.168.1.12][UDP:8472][VXLAN][åŸå§‹å°åŒ…]
```

## 3. ç¶²è·¯ä»‹é¢åˆ†æ

### æ¯å€‹ç¯€é»çš„ç¶²è·¯ä»‹é¢

#### Control Plane ç¯€é»
- **eth0**: ç¯€é»ä¸»è¦ç¶²è·¯ä»‹é¢ (192.168.x.x)
- **flannel.1**: VXLAN ä»‹é¢ (10.244.0.0)
- **cni0**: Pod ç¶²è·¯æ©‹æ¥å™¨ (10.244.0.1)

#### Worker ç¯€é»
- **eth0**: ç¯€é»ä¸»è¦ç¶²è·¯ä»‹é¢
- **flannel.1**: VXLAN ä»‹é¢
- **cni0**: Pod ç¶²è·¯æ©‹æ¥å™¨
- **vethXXX**: Pod çš„ veth pair ä»‹é¢

## 4. è·¯ç”±è¡¨åˆ†æ

### å…¸å‹çš„ç¯€é»è·¯ç”±è¡¨

```bash
# ç¯€é»å…§éƒ¨ Pod å­ç¶²
10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1

# å…¶ä»–ç¯€é»çš„ Pod å­ç¶² (é€šé VXLAN)
10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink
10.244.3.0/24 via 10.244.3.0 dev flannel.1 onlink

# é è¨­è·¯ç”±
default via 192.168.65.1 dev eth0
```

## 5. Service ç¶²è·¯åˆ†æ

### ClusterIP Service å·¥ä½œåŸç†

1. **kube-proxy** ç›£è½ Service è®ŠåŒ–
2. åœ¨æ¯å€‹ç¯€é»å‰µå»º **iptables** è¦å‰‡
3. å°åŒ…åˆ°é” Service IP æ™‚è¢« **DNAT** åˆ°å¾Œç«¯ Pod IP
4. ä½¿ç”¨ Flannel ç¶²è·¯é€²è¡Œ Pod é–“é€šè¨Š

### iptables è¦å‰‡ç¯„ä¾‹

```bash
# DNAT è¦å‰‡ (Service -> Pod)
-A KUBE-SERVICES -d 10.96.1.100/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-XXX

# è² è¼‰å¹³è¡¡è¦å‰‡
-A KUBE-SVC-XXX -m statistic --mode random --probability 0.5 -j KUBE-SEP-POD1
-A KUBE-SVC-XXX -j KUBE-SEP-POD2

# Pod ç«¯é»è¦å‰‡
-A KUBE-SEP-POD1 -p tcp -m tcp -j DNAT --to-destination 10.244.1.2:80
-A KUBE-SEP-POD2 -p tcp -m tcp -j DNAT --to-destination 10.244.2.2:80
```

## 6. ç¶²è·¯æ€§èƒ½è€ƒé‡

### VXLAN é–‹éŠ·

- **MTU æ¸›å°‘**: åŸå§‹ 1500 bytes -> 1450 bytes (VXLAN header 50 bytes)
- **CPU é–‹éŠ·**: å°è£/è§£å°è£éœ€è¦é¡å¤– CPU è³‡æº
- **å»¶é²å¢åŠ **: ç´„ 0.1-0.5ms é¡å¤–å»¶é²

### å„ªåŒ–å»ºè­°

1. **èª¿æ•´ MTU**: åœ¨æ”¯æ´çš„ç’°å¢ƒä¸­ä½¿ç”¨ Jumbo Frames
2. **è€ƒæ…® host-gw**: åœ¨ Layer 2 é€£é€šçš„ç’°å¢ƒä¸­ä½¿ç”¨ host-gw æ¨¡å¼
3. **ç›£æ§ç¶²è·¯**: ä½¿ç”¨ Prometheus + Grafana ç›£æ§ç¶²è·¯æŒ‡æ¨™

## 7. æ•…éšœæ’é™¤æŒ‡å—

### å¸¸è¦‹å•é¡Œ

#### Pod ç„¡æ³•é€šè¨Š
1. æª¢æŸ¥ Flannel Pod ç‹€æ…‹
2. ç¢ºèªç¯€é»å­ç¶²åˆ†é…
3. æª¢æŸ¥ VXLAN ä»‹é¢ç‹€æ…‹
4. é©—è­‰è·¯ç”±è¡¨é…ç½®

#### ç¶²è·¯æ€§èƒ½å•é¡Œ
1. æª¢æŸ¥ MTU è¨­å®š
2. ç›£æ§ CPU ä½¿ç”¨ç‡
3. åˆ†æç¶²è·¯å»¶é²
4. è€ƒæ…®å¾Œç«¯æ¨¡å¼åˆ‡æ›

### è¨ºæ–·å‘½ä»¤

```bash
# æª¢æŸ¥ Flannel ç‹€æ…‹
kubectl get pods -n kube-flannel
kubectl logs -n kube-flannel -l app=flannel

# æª¢æŸ¥ç¯€é»ç¶²è·¯
ip addr show flannel.1
ip route | grep flannel
ip neigh show dev flannel.1

# æª¢æŸ¥ Pod ç¶²è·¯
kubectl exec -it <pod> -- ip addr show eth0
kubectl exec -it <pod> -- ip route

# æ¸¬è©¦é€£é€šæ€§
kubectl exec -it <pod> -- ping <target-ip>
kubectl exec -it <pod> -- traceroute <target-ip>
```

## 8. å®‰å…¨è€ƒé‡

### ç¶²è·¯éš”é›¢

Flannel æœ¬èº«ä¸æä¾›ç¶²è·¯ç­–ç•¥åŠŸèƒ½ï¼Œéœ€è¦é¡å¤–çš„è§£æ±ºæ–¹æ¡ˆï¼š

1. **Calico**: æä¾›ç¶²è·¯ç­–ç•¥åŠŸèƒ½
2. **Cilium**: åŸºæ–¼ eBPF çš„ç¶²è·¯å’Œå®‰å…¨
3. **Weave Net**: å…§å»ºç¶²è·¯ç­–ç•¥æ”¯æ´

### åŠ å¯†

VXLAN æµé‡é è¨­ä¸åŠ å¯†ï¼Œåœ¨éœ€è¦çš„ç’°å¢ƒä¸­å¯ä»¥è€ƒæ…®ï¼š

1. **IPSec**: ç¯€é»é–“ IPSec éš§é“
2. **WireGuard**: ç¾ä»£ VPN è§£æ±ºæ–¹æ¡ˆ
3. **Service Mesh**: æ‡‰ç”¨å±¤åŠ å¯† (å¦‚ Istio)

EOF

echo "âœ… è©³ç´°åˆ†æå ±å‘Šå·²ç”Ÿæˆ: network-analysis/detailed-analysis.md"
```

### åŸ·è¡Œç¶²è·¯æ‹“æ’²åˆ†æ

```bash
# çµ¦è…³æœ¬åŸ·è¡Œæ¬Šé™
chmod +x generate-network-topology.sh
chmod +x analyze-flannel-network.sh

# åŸ·è¡Œåˆ†æ
./analyze-flannel-network.sh
./generate-network-topology.sh
```

---

## ğŸ“Š å¯¦ä½œ 4ï¼šç¶²è·¯æ€§èƒ½æ¸¬è©¦èˆ‡æ¯”è¼ƒ

### ç¶²è·¯æ€§èƒ½æ¸¬è©¦è…³æœ¬

```bash
#!/bin/bash
# network-performance-test.sh

set -e

echo "âš¡ Flannel ç¶²è·¯æ€§èƒ½æ¸¬è©¦"

# ç¢ºä¿æ¸¬è©¦ç’°å¢ƒå­˜åœ¨
if ! kubectl get namespace network-test &>/dev/null; then
  echo "âŒ è«‹å…ˆåŸ·è¡Œ test-pod-networking.sh å‰µå»ºæ¸¬è©¦ç’°å¢ƒ"
  exit 1
fi

# å‰µå»ºæ€§èƒ½æ¸¬è©¦ç›®éŒ„
mkdir -p performance-results

# ç²å–æ¸¬è©¦ Pod
DEBUG_POD=$(kubectl get pods -n network-test -l app=network-debug -o jsonpath='{.items[0].metadata.name}')
ZONE_A_POD=$(kubectl get pods -n network-test -l zone=zone-a -o jsonpath='{.items[0].metadata.name}')
ZONE_B_POD=$(kubectl get pods -n network-test -l zone=zone-b -o jsonpath='{.items[0].metadata.name}')
ZONE_C_POD=$(kubectl get pods -n network-test -l zone=zone-c -o jsonpath='{.items[0].metadata.name}')

ZONE_A_IP=$(kubectl get pods -n network-test -l zone=zone-a -o jsonpath='{.items[0].status.podIP}')
ZONE_B_IP=$(kubectl get pods -n network-test -l zone=zone-b -o jsonpath='{.items[0].status.podIP}')
ZONE_C_IP=$(kubectl get pods -n network-test -l zone=zone-c -o jsonpath='{.items[0].status.podIP}')

echo "ğŸ” æ¸¬è©¦ç’°å¢ƒ:"
echo "Debug Pod: $DEBUG_POD"
echo "Zone A Pod: $ZONE_A_POD ($ZONE_A_IP)"
echo "Zone B Pod: $ZONE_B_POD ($ZONE_B_IP)"
echo "Zone C Pod: $ZONE_C_POD ($ZONE_C_IP)"

# 1. å»¶é²æ¸¬è©¦
echo ""
echo "ğŸ“¡ å»¶é²æ¸¬è©¦..."

echo "=== åŒç¯€é»é€šè¨Šå»¶é² ===" | tee performance-results/latency-test.txt
kubectl exec -n network-test $DEBUG_POD -- ping -c 10 $ZONE_A_IP | tail -1 | tee -a performance-results/latency-test.txt

echo "" | tee -a performance-results/latency-test.txt
echo "=== è·¨ç¯€é»é€šè¨Šå»¶é² (Zone A -> Zone B) ===" | tee -a performance-results/latency-test.txt
kubectl exec -n network-test $DEBUG_POD -- ping -c 10 $ZONE_B_IP | tail -1 | tee -a performance-results/latency-test.txt

echo "" | tee -a performance-results/latency-test.txt
echo "=== è·¨ç¯€é»é€šè¨Šå»¶é² (Zone A -> Zone C) ===" | tee -a performance-results/latency-test.txt
kubectl exec -n network-test $DEBUG_POD -- ping -c 10 $ZONE_C_IP | tail -1 | tee -a performance-results/latency-test.txt

# 2. é »å¯¬æ¸¬è©¦
echo ""
echo "ğŸš€ é »å¯¬æ¸¬è©¦..."

# å®‰è£ iperf3 åˆ°æ¸¬è©¦ Pod
echo "å®‰è£ iperf3..."
kubectl exec -n network-test $ZONE_A_POD -- sh -c "apk add --no-cache iperf3" &>/dev/null || true
kubectl exec -n network-test $ZONE_B_POD -- sh -c "apk add --no-cache iperf3" &>/dev/null || true
kubectl exec -n network-test $ZONE_C_POD -- sh -c "apk add --no-cache iperf3" &>/dev/null || true

# å•Ÿå‹• iperf3 æœå‹™å™¨
echo "å•Ÿå‹• iperf3 æœå‹™å™¨..."
kubectl exec -n network-test $ZONE_B_POD -- sh -c "iperf3 -s -D" &>/dev/null || true
kubectl exec -n network-test $ZONE_C_POD -- sh -c "iperf3 -s -D" &>/dev/null || true

sleep 3

echo "=== è·¨ç¯€é»é »å¯¬æ¸¬è©¦ (Zone A -> Zone B) ===" | tee performance-results/bandwidth-test.txt
kubectl exec -n network-test $ZONE_A_POD -- iperf3 -c $ZONE_B_IP -t 10 -f M | tee -a performance-results/bandwidth-test.txt

echo "" | tee -a performance-results/bandwidth-test.txt
echo "=== è·¨ç¯€é»é »å¯¬æ¸¬è©¦ (Zone A -> Zone C) ===" | tee -a performance-results/bandwidth-test.txt
kubectl exec -n network-test $ZONE_A_POD -- iperf3 -c $ZONE_C_IP -t 10 -f M | tee -a performance-results/bandwidth-test.txt

# 3. HTTP æ€§èƒ½æ¸¬è©¦
echo ""
echo "ğŸŒ HTTP æ€§èƒ½æ¸¬è©¦..."

# å®‰è£ wrk åˆ° debug pod
kubectl exec -n network-test $DEBUG_POD -- sh -c "apk add --no-cache wrk" &>/dev/null || true

echo "=== HTTP æ€§èƒ½æ¸¬è©¦ (åŒç¯€é») ===" | tee performance-results/http-test.txt
kubectl exec -n network-test $DEBUG_POD -- wrk -t4 -c100 -d30s --latency http://$ZONE_A_IP/ | tee -a performance-results/http-test.txt

echo "" | tee -a performance-results/http-test.txt
echo "=== HTTP æ€§èƒ½æ¸¬è©¦ (è·¨ç¯€é») ===" | tee -a performance-results/http-test.txt
kubectl exec -n network-test $DEBUG_POD -- wrk -t4 -c100 -d30s --latency http://$ZONE_B_IP/ | tee -a performance-results/http-test.txt

# 4. DNS è§£ææ€§èƒ½æ¸¬è©¦
echo ""
echo "ğŸ” DNS è§£ææ€§èƒ½æ¸¬è©¦..."

echo "=== DNS è§£ææ¸¬è©¦ ===" | tee performance-results/dns-test.txt

# æ¸¬è©¦ Service DNS è§£æ
for i in {1..10}; do
  echo "æ¸¬è©¦ $i:" | tee -a performance-results/dns-test.txt
  kubectl exec -n network-test $DEBUG_POD -- time nslookup test-app-zone-a-service.network-test.svc.cluster.local 2>&1 | grep real | tee -a performance-results/dns-test.txt
done

# 5. ä¸¦ç™¼é€£æ¥æ¸¬è©¦
echo ""
echo "ğŸ”„ ä¸¦ç™¼é€£æ¥æ¸¬è©¦..."

echo "=== ä¸¦ç™¼é€£æ¥æ¸¬è©¦ ===" | tee performance-results/concurrent-test.txt

# å‰µå»ºå¤šå€‹ä¸¦ç™¼é€£æ¥
for i in {1..5}; do
  kubectl exec -n network-test $DEBUG_POD -- sh -c "curl -s http://$ZONE_B_IP/ &" &
done

wait

echo "ä¸¦ç™¼æ¸¬è©¦å®Œæˆ" | tee -a performance-results/concurrent-test.txt

# 6. ç”Ÿæˆæ€§èƒ½å ±å‘Š
echo ""
echo "ğŸ“Š ç”Ÿæˆæ€§èƒ½å ±å‘Š..."

cat > performance-results/performance-summary.md << EOF
# Flannel CNI ç¶²è·¯æ€§èƒ½æ¸¬è©¦å ±å‘Š

## æ¸¬è©¦ç’°å¢ƒ
- **CNI**: Flannel (VXLAN æ¨¡å¼)
- **Kubernetes ç‰ˆæœ¬**: $(kubectl version --short | grep Server)
- **ç¯€é»æ•¸é‡**: $(kubectl get nodes --no-headers | wc -l)
- **æ¸¬è©¦æ™‚é–“**: $(date)

## æ¸¬è©¦çµæœæ‘˜è¦

### å»¶é²æ¸¬è©¦
$(cat performance-results/latency-test.txt)

### é »å¯¬æ¸¬è©¦
$(cat performance-results/bandwidth-test.txt)

### HTTP æ€§èƒ½æ¸¬è©¦
$(cat performance-results/http-test.txt)

### DNS è§£ææ¸¬è©¦
$(cat performance-results/dns-test.txt)

## æ€§èƒ½åˆ†æ

### å»¶é²åˆ†æ
- **åŒç¯€é»é€šè¨Š**: é€šå¸¸ < 0.1ms
- **è·¨ç¯€é»é€šè¨Š**: é€šå¸¸ 0.1-1ms (å–æ±ºæ–¼ VXLAN é–‹éŠ·)

### é »å¯¬åˆ†æ
- **VXLAN é–‹éŠ·**: ç´„ 5-10% çš„é »å¯¬æå¤±
- **MTU å½±éŸ¿**: 1450 bytes (ç›¸æ¯”åŸå§‹ 1500 bytes)

### å„ªåŒ–å»ºè­°
1. åœ¨ Layer 2 é€£é€šç’°å¢ƒè€ƒæ…®ä½¿ç”¨ host-gw æ¨¡å¼
2. èª¿æ•´ MTU è¨­å®šä»¥æ”¯æ´ Jumbo Frames
3. ç›£æ§ CPU ä½¿ç”¨ç‡ï¼ŒVXLAN å°è£éœ€è¦é¡å¤–è¨ˆç®—è³‡æº

## èˆ‡å…¶ä»– CNI æ¯”è¼ƒ

| CNI | å»¶é² | é »å¯¬ | è¤‡é›œåº¦ | åŠŸèƒ½ |
|-----|------|------|--------|------|
| Flannel (VXLAN) | ä¸­ç­‰ | ä¸­ç­‰ | ä½ | åŸºæœ¬ |
| Flannel (host-gw) | ä½ | é«˜ | ä½ | åŸºæœ¬ |
| Calico | ä½ | é«˜ | ä¸­ç­‰ | è±å¯Œ |
| Cilium | ä½ | é«˜ | é«˜ | æœ€è±å¯Œ |
| Weave | ä¸­ç­‰ | ä¸­ç­‰ | ä¸­ç­‰ | ä¸­ç­‰ |

EOF

echo "âœ… æ€§èƒ½æ¸¬è©¦å®Œæˆï¼çµæœä¿å­˜åœ¨ performance-results/ ç›®éŒ„"
echo ""
echo "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:"
ls -la performance-results/
```

---

## ğŸ”§ æ•…éšœæ’é™¤èˆ‡æœ€ä½³å¯¦è¸

### å¸¸è¦‹å•é¡Œè¨ºæ–·

```bash
#!/bin/bash
# flannel-troubleshooting.sh

echo "ğŸ©º Flannel CNI æ•…éšœæ’é™¤æŒ‡å—"

# 1. æª¢æŸ¥ Flannel åŸºæœ¬ç‹€æ…‹
echo "1ï¸âƒ£ æª¢æŸ¥ Flannel åŸºæœ¬ç‹€æ…‹..."

echo "Flannel DaemonSet ç‹€æ…‹:"
kubectl get daemonset -n kube-flannel

echo ""
echo "Flannel Pod ç‹€æ…‹:"
kubectl get pods -n kube-flannel -o wide

echo ""
echo "ç¯€é»å°±ç·’ç‹€æ…‹:"
kubectl get nodes

# 2. æª¢æŸ¥ç¶²è·¯é…ç½®
echo ""
echo "2ï¸âƒ£ æª¢æŸ¥ç¶²è·¯é…ç½®..."

echo "Flannel ConfigMap:"
kubectl get configmap kube-flannel-cfg -n kube-flannel -o yaml | grep -A 10 -B 5 "net-conf.json"

echo ""
echo "ç¯€é» Pod CIDR åˆ†é…:"
kubectl get nodes -o custom-columns="NAME:.metadata.name,POD_CIDR:.spec.podCIDR"

# 3. æª¢æŸ¥ Pod ç¶²è·¯ç‹€æ…‹
echo ""
echo "3ï¸âƒ£ æª¢æŸ¥ Pod ç¶²è·¯ç‹€æ…‹..."

echo "Pending ç‹€æ…‹çš„ Pod:"
kubectl get pods -A | grep Pending || echo "ç„¡ Pending Pod"

echo ""
echo "ç¶²è·¯ç›¸é—œéŒ¯èª¤çš„ Pod:"
kubectl get events -A | grep -i "network\|cni\|flannel" | tail -10 || echo "ç„¡ç›¸é—œéŒ¯èª¤"

# 4. æª¢æŸ¥ç¯€é»ç¶²è·¯ä»‹é¢
echo ""
echo "4ï¸âƒ£ æª¢æŸ¥ç¯€é»ç¶²è·¯ä»‹é¢..."

NODES=($(kubectl get nodes -o jsonpath='{.items[*].metadata.name}'))
for NODE in "${NODES[@]}"; do
  echo ""
  echo "=== ç¯€é» $NODE ==="
  
  echo "VXLAN ä»‹é¢ç‹€æ…‹:"
  docker exec $NODE ip link show flannel.1 2>/dev/null || echo "âŒ VXLAN ä»‹é¢ä¸å­˜åœ¨"
  
  echo "CNI æ©‹æ¥å™¨ç‹€æ…‹:"
  docker exec $NODE ip link show cni0 2>/dev/null || echo "âŒ CNI æ©‹æ¥å™¨ä¸å­˜åœ¨"
  
  echo "è·¯ç”±è¡¨ (Flannel ç›¸é—œ):"
  docker exec $NODE ip route | grep -E "(10\.244|flannel)" || echo "âŒ ç„¡ Flannel è·¯ç”±"
done

# 5. ç¶²è·¯é€£é€šæ€§æ¸¬è©¦
echo ""
echo "5ï¸âƒ£ ç¶²è·¯é€£é€šæ€§æ¸¬è©¦..."

if kubectl get namespace network-test &>/dev/null; then
  DEBUG_POD=$(kubectl get pods -n network-test -l app=network-debug -o jsonpath='{.items[0].metadata.name}')
  
  if [ ! -z "$DEBUG_POD" ]; then
      echo "ä½¿ç”¨èª¿è©¦ Pod: $DEBUG_POD"
      
      # æ¸¬è©¦ DNS
      echo "DNS è§£ææ¸¬è©¦:"
      kubectl exec -n network-test $DEBUG_POD -- nslookup kubernetes.default.svc.cluster.local || echo "âŒ DNS è§£æå¤±æ•—"
      
      # æ¸¬è©¦ Service é€£é€šæ€§
      echo ""
      echo "Service é€£é€šæ€§æ¸¬è©¦:"
      kubectl exec -n network-test $DEBUG_POD -- curl -s --connect-timeout 5 http://kubernetes.default.svc.cluster.local:443 &>/dev/null && echo "âœ… Service é€£é€šæ­£å¸¸" || echo "âŒ Service é€£é€šå¤±æ•—"
  fi
else
  echo "âš ï¸  æ¸¬è©¦ç’°å¢ƒä¸å­˜åœ¨ï¼Œè·³éé€£é€šæ€§æ¸¬è©¦"
fi

# 6. å¸¸è¦‹å•é¡Œè§£æ±ºæ–¹æ¡ˆ
cat << 'EOF'

ğŸ”§ å¸¸è¦‹å•é¡Œè§£æ±ºæ–¹æ¡ˆ:

1. Pod ç„¡æ³•å•Ÿå‹• (Pending ç‹€æ…‹)
 - æª¢æŸ¥ Flannel DaemonSet æ˜¯å¦æ­£å¸¸é‹è¡Œ
 - ç¢ºèªç¯€é»å·²åˆ†é… Pod CIDR
 - æª¢æŸ¥ CNI é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨

2. Pod é–“ç„¡æ³•é€šè¨Š
 - æª¢æŸ¥ VXLAN ä»‹é¢æ˜¯å¦æ­£å¸¸
 - ç¢ºèªè·¯ç”±è¡¨é…ç½®æ­£ç¢º
 - æª¢æŸ¥ iptables è¦å‰‡

3. DNS è§£æå¤±æ•—
 - æª¢æŸ¥ CoreDNS Pod ç‹€æ…‹
 - ç¢ºèª Service CIDR é…ç½®
 - æª¢æŸ¥ kube-proxy é‹è¡Œç‹€æ…‹

4. ç¶²è·¯æ€§èƒ½å•é¡Œ
 - æª¢æŸ¥ MTU è¨­å®š
 - è€ƒæ…®åˆ‡æ›åˆ° host-gw æ¨¡å¼
 - ç›£æ§ç¯€é» CPU ä½¿ç”¨ç‡

5. Flannel Pod é‡å•Ÿ
 - æª¢æŸ¥ç¯€é»è³‡æºä½¿ç”¨æƒ…æ³
 - æŸ¥çœ‹ Flannel Pod æ—¥èªŒ
 - ç¢ºèª etcd æˆ– k8s API é€£é€šæ€§

ğŸ“ è¨ºæ–·å‘½ä»¤:
kubectl logs -n kube-flannel -l app=flannel
kubectl describe node <node-name>
kubectl get events -A --sort-by='.lastTimestamp'
docker exec <node-name> ip addr show
docker exec <node-name> ip route
EOF
```

### Flannel æœ€ä½³å¯¦è¸

```yaml
# flannel-best-practices.yaml

# 1. ç”Ÿç”¢ç’°å¢ƒ Flannel é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
name: kube-flannel-cfg
namespace: kube-flannel
labels:
  tier: node
  app: flannel
data:
cni-conf.json: |
  {
    "name": "cbr0",
    "cniVersion": "0.3.1",
    "plugins": [
      {
        "type": "flannel",
        "delegate": {
          "hairpinMode": true,
          "isDefaultGateway": true
        }
      },
      {
        "type": "portmap",
        "capabilities": {
          "portMappings": true
        }
      }
    ]
  }
net-conf.json: |
  {
    "Network": "10.244.0.0/16",
    "Backend": {
      "Type": "vxlan",
      "Port": 8472,
      "VNI": 1,
      "DirectRouting": false
    }
  }

---
# 2. ç”Ÿç”¢ç’°å¢ƒ DaemonSet é…ç½®
apiVersion: apps/v1
kind: DaemonSet
metadata:
name: kube-flannel-ds
namespace: kube-flannel
labels:
  tier: node
  app: flannel
spec:
selector:
  matchLabels:
    app: flannel
template:
  metadata:
    labels:
      tier: node
      app: flannel
  spec:
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/os
              operator: In
              values:
              - linux
    hostNetwork: true
    priorityClassName: system-node-critical
    tolerations:
    - operator: Exists
      effect: NoSchedule
    serviceAccountName: flannel
    initContainers:
    - name: install-cni-plugin
      image: docker.io/flannel/flannel-cni-plugin:v1.1.2
      command:
      - cp
      args:
      - -f
      - /flannel
      - /opt/cni/bin/flannel
      volumeMounts:
      - name: cni-plugin
        mountPath: /opt/cni/bin
      resources:
        requests:
          cpu: "100m"
          memory: "50Mi"
        limits:
          cpu: "100m"
          memory: "50Mi"
    - name: install-cni
      image: docker.io/flannel/flannel:v0.22.0
      command:
      - cp
      args:
      - -f
      - /etc/kube-flannel/cni-conf.json
      - /etc/cni/net.d/10-flannel.conflist
      volumeMounts:
      - name: cni
        mountPath: /etc/cni/net.d
      - name: flannel-cfg
        mountPath: /etc/kube-flannel/
      resources:
        requests:
          cpu: "100m"
          memory: "50Mi"
        limits:
          cpu: "100m"
          memory: "50Mi"
    containers:
    - name: kube-flannel
      image: docker.io/flannel/flannel:v0.22.0
      command:
      - /opt/bin/flanneld
      args:
      - --ip-masq
      - --kube-subnet-mgr
      - --iface-regex=^(eth0|ens.*|enp.*|eno.*)$
      resources:
        requests:
          cpu: "100m"
          memory: "50Mi"
        limits:
          cpu: "100m"
          memory: "50Mi"
      securityContext:
        privileged: false
        capabilities:
          add: ["NET_ADMIN", "NET_RAW"]
      env:
      - name: POD_NAME
        valueFrom:
          fieldRef:
            fieldPath: metadata.name
      - name: POD_NAMESPACE
        valueFrom:
          fieldRef:
            fieldPath: metadata.namespace
      - name: EVENT_QUEUE_DEPTH
        value: "5000"
      volumeMounts:
      - name: run
        mountPath: /run/flannel
      - name: flannel-cfg
        mountPath: /etc/kube-flannel/
      - name: xtables-lock
        mountPath: /run/xtables.lock
      livenessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - 'test -f /run/flannel/subnet.env'
        initialDelaySeconds: 10
        periodSeconds: 10
        timeoutSeconds: 5
      readinessProbe:
        exec:
          command:
          - /bin/sh
          - -c
          - 'test -f /run/flannel/subnet.env'
        initialDelaySeconds: 5
        periodSeconds: 5
        timeoutSeconds: 5
    volumes:
    - name: run
      hostPath:
        path: /run/flannel
    - name: cni-plugin
      hostPath:
        path: /opt/cni/bin
    - name: cni
      hostPath:
        path: /etc/cni/net.d
    - name: flannel-cfg
      configMap:
        name: kube-flannel-cfg
    - name: xtables-lock
      hostPath:
        path: /run/xtables.lock
        type: FileOrCreate
```

---

## ğŸ“š å­¸ç¿’ç¸½çµ

### ä»Šå¤©å­¸åˆ°çš„é‡é»

1. **CNI æ¦‚å¿µèˆ‡æ¶æ§‹**
 - CNI æ¨™æº–åŒ–ä»‹é¢çš„é‡è¦æ€§
 - æ’ä»¶åŒ–ç¶²è·¯æ¶æ§‹çš„å„ªå‹¢
 - èˆ‡å®¹å™¨é‹è¡Œæ™‚çš„æ•´åˆæ–¹å¼

2. **Flannel CNI æ·±å…¥ç†è§£**
 - VXLAN å°è£æŠ€è¡“åŸç†
 - å­ç¶²åˆ†é…èˆ‡ç®¡ç†æ©Ÿåˆ¶
 - ä¸åŒå¾Œç«¯æ¨¡å¼çš„ç‰¹é»èˆ‡é¸æ“‡

3. **ç¶²è·¯é€šè¨Šæµç¨‹**
 - Pod é–“é€šè¨Šçš„å®Œæ•´è·¯å¾‘
 - Service ç¶²è·¯çš„å¯¦ç¾åŸç†
 - 